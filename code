import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Generating a simple synthetic dataset for cats and dogs
# Assume features like 'Weight', 'Height', and 'Length'
np.random.seed(0)
data = {
    'Weight': np.random.normal(loc=10, scale=2, size=100).tolist() + np.random.normal(loc=20, scale=3, size=100).tolist(),
    'Height': np.random.normal(loc=20, scale=4, size=100).tolist() + np.random.normal(loc=30, scale=5, size=100).tolist(),
    'Length': np.random.normal(loc=30, scale=5, size=100).tolist() + np.random.normal(loc=40, scale=6, size=100).tolist(),
    'Label': [0] * 100 + [1] * 100  # 0 for Cat, 1 for Dog
}

# Creating DataFrame
df = pd.DataFrame(data)
X = df[['Weight', 'Height', 'Length']]
y = df['Label']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Training the SVM model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Making predictions
y_pred = svm_model.predict(X_test)

# Output the results
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Visualizing the results (using only two features for simplicity)
plt.figure(figsize=(10, 6))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.7)
plt.title("SVM Classification of Cats and Dogs")
plt.xlabel("Weight")
plt.ylabel("Height")
plt.show()


  
  
  
                                                                                        Code Explanation:




(1)Importing Libraries:

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt


#NumPy: Used for numerical operations and handling arrays.
#pandas: Used for data manipulation and analysis, particularly for handling data in DataFrame format.
#scikit-learn: A machine learning library that provides tools for model training (SVC), evaluation (classification_report, confusion_matrix), and data splitting (train_test_split).
#matplotlib: A plotting library used to visualize data.

(2)Generating a Synthetic Dataset:[u can also create your own dataset]

np.random.seed(0)
data = {
    'Weight': np.random.normal(loc=10, scale=2, size=100).tolist() + np.random.normal(loc=20, scale=3, size=100).tolist(),
    'Height': np.random.normal(loc=20, scale=4, size=100).tolist() + np.random.normal(loc=30, scale=5, size=100).tolist(),
    'Length': np.random.normal(loc=30, scale=5, size=100).tolist() + np.random.normal(loc=40, scale=6, size=100).tolist(),
    'Label': [0] * 100 + [1] * 100  # 0 for Cat, 1 for Dog
}

#Random Seed: np.random.seed(0) ensures reproducibility by initializing the random number generator.
#Dataset Creation: Three features (Weight, Height, and Length) are generated for 100 cats (label 0) and 100 dogs (label 1) using normal distributions with specified means (loc) and standard deviations (scale).

(3)Creating a DataFrame:

df = pd.DataFrame(data)
X = df[['Weight', 'Height', 'Length']]
y = df['Label']

#DataFrame Creation: The synthetic dataset is converted into a pandas DataFrame for easier manipulation.
#Feature and Label Separation: The features (X) and labels (y) are extracted for use in model training.

4)Splitting the Data:

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Data Splitting: The dataset is split into training (70%) and testing (30%) sets. This allows the model to learn from one portion of the data while evaluating its performance on a separate unseen portion.

(5)Standardizing the Features:

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Feature Standardization: The StandardScaler standardizes the features by removing the mean and scaling to unit variance. This step is crucial for SVM, as it ensures that all features contribute equally 
 to the distance calculations during classification.

(6)Training the SVM Model:

svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

#Model Initialization: An SVM classifier is created with a linear kernel. The linear kernel is often a good choice for linearly separable data.
#Model Training: The SVM model is trained on the standardized training data (X_train and y_train).

(7)Making Predictions:

y_pred = svm_model.predict(X_test)

#Predictions: The trained model is used to predict the labels for the testing set (X_test), resulting in the predicted labels (y_pred)

(8)Evaluating the Model:

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

#Confusion Matrix: Displays the true positives, false positives, true negatives, and false negatives, providing insight into the classification performance.
#Classification Report: Contains precision, recall, F1-score, and support for each class, giving a comprehensive view of the model's effectiveness.

(9)Visualizing the Results:

plt.figure(figsize=(10, 6))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.7)
plt.title("SVM Classification of Cats and Dogs")
plt.xlabel("Weight")
plt.ylabel("Height")
plt.show()

#Plotting: A scatter plot is created to visualize the classification results, using Weight and Height as the x and y axes, respectively. The predicted labels are color-coded, allowing for a visual assessment 
 of how well the model separates the two classes.

>>Conclusion:
This code effectively demonstrates the process of creating a synthetic dataset, training a SVM classifier, evaluating its performance, and visualizing the results. It provides a solid foundation
for understanding binary classification tasks using machine learning techniques.
